import os, streamlit as st, snowflake.connector as sf
import pandas as pd, numpy as np, matplotlib.pyplot as plt, seaborn as sns
from sklearn.preprocessing import StandardScaler; from sklearn.decomposition import PCA

os.environ["STREAMLIT_SERVER_HEALTH_CHECK_ENABLED"] = "false"
st.set_page_config(page_title="ì„œìš¸ì‹œ ê°ì„± ì§€ìˆ˜ ëŒ€ì‹œë³´ë“œ", layout="wide"); sns.set_style("whitegrid")
import matplotlib; matplotlib.rcParams["font.family"]="Malgun Gothic"; matplotlib.rcParams["axes.unicode_minus"]=False
st.markdown('<style>*{font-family:"Malgun Gothic",sans-serif!important;}</style>',unsafe_allow_html=True)
st.write("ğŸš€ Streamlit ì•± ì‹œì‘!")

def get_conn():
    return sf.connect(user=st.secrets["snowflake"]["user"],
                      password=st.secrets["snowflake"]["password"],
                      account=st.secrets["snowflake"]["account"],
                      warehouse="COMPUTE_WH",
                      ocsp_fail_open=True,insecure_mode=True)

@st.cache_data(show_spinner=False)
def load(q):
    with get_conn() as c:
        cur=c.cursor(); cur.execute(q)
        df=pd.DataFrame(cur.fetchall(),columns=[x[0] for x in cur.description]); cur.close()
    return df

BASE="SEOUL_DISTRICTLEVEL_DATA_FLOATING_POPULATION_CONSUMPTION_AND_ASSETS.GRANDATA"
Q_FP   =f"SELECT * FROM {BASE}.FLOATING_POPULATION_INFO"
Q_CARD =f"SELECT * FROM {BASE}.CARD_SALES_INFO"
Q_ASSET=f"SELECT * FROM {BASE}.ASSET_INCOME_INFO"
Q_SCCO =f"SELECT * FROM {BASE}.M_SCCO_MST"

@st.cache_data(show_spinner=True)
def preprocess():
    fp,card,asset,scco=load(Q_FP),load(Q_CARD),load(Q_ASSET),load(Q_SCCO)
    fp=fp.sample(frac=0.05,random_state=42)                      # 5 %ë§Œ ë©”ëª¨ë¦¬ íƒ‘ì¬
    fp_card=pd.merge(fp,card,on=[
        "STANDARD_YEAR_MONTH","DISTRICT_CODE","AGE_GROUP","GENDER",
        "TIME_SLOT","WEEKDAY_WEEKEND"],how="left")               # â† left join
    data=pd.merge(fp_card,asset,
        on=["STANDARD_YEAR_MONTH","DISTRICT_CODE","AGE_GROUP","GENDER"],
        how="left")                                              # â† left join

    scco_map=scco.drop_duplicates("DISTRICT_CODE").set_index("DISTRICT_CODE")["DISTRICT_KOR_NAME"]
    data["DISTRICT_KOR_NAME"]=data["DISTRICT_CODE"].map(scco_map)

    data["ì „ì²´ì¸êµ¬"]=data["RESIDENTIAL_POPULATION"]+data["WORKING_POPULATION"]+data["VISITING_POPULATION"]
    data["ì—”í„°ì „ì²´ë§¤ì¶œ"]=(
        data["FOOD_SALES"]+data["COFFEE_SALES"]+data["BEAUTY_SALES"]+
        data["ENTERTAINMENT_SALES"]+data["SPORTS_CULTURE_LEISURE_SALES"]+
        data["TRAVEL_SALES"]+data["CLOTHING_ACCESSORIES_SALES"])
    data["ì†Œë¹„í™œë ¥ì§€ìˆ˜"]=data["ì—”í„°ì „ì²´ë§¤ì¶œ"]/data["ì „ì²´ì¸êµ¬"].replace(0,np.nan)
    data["ìœ ì…ì§€ìˆ˜"]=data["VISITING_POPULATION"]/(
        data["RESIDENTIAL_POPULATION"]+data["WORKING_POPULATION"]).replace(0,np.nan)
    data["ì—”í„°ë§¤ì¶œë¹„ìœ¨"]=data["ì—”í„°ì „ì²´ë§¤ì¶œ"]/data["TOTAL_SALES"].replace(0,np.nan)

    cnt_cols=[
        "FOOD_COUNT","COFFEE_COUNT","BEAUTY_COUNT","ENTERTAINMENT_COUNT",
        "SPORTS_CULTURE_LEISURE_COUNT","TRAVEL_COUNT","CLOTHING_ACCESSORIES_COUNT"]
    data["ì—”í„°ì „ì²´ë°©ë¬¸ììˆ˜"]=data[cnt_cols].sum(axis=1)
    data["ì—”í„°ë°©ë¬¸ìë¹„ìœ¨"]=data["ì—”í„°ì „ì²´ë°©ë¬¸ììˆ˜"]/data["TOTAL_COUNT"].replace(0,np.nan)
    data["ì—”í„°í™œë™ë°€ë„"]=data["ì—”í„°ì „ì²´ë§¤ì¶œ"]/data["ì „ì²´ì¸êµ¬"].replace(0,np.nan)
    data["ì—”í„°ë§¤ì¶œë°€ë„"]=data["ì—”í„°ì „ì²´ë§¤ì¶œ"]/data["ì—”í„°ì „ì²´ë°©ë¬¸ììˆ˜"].replace(0,np.nan)

    emo_vars=["ì—”í„°ì „ì²´ë§¤ì¶œ","ì†Œë¹„í™œë ¥ì§€ìˆ˜","ìœ ì…ì§€ìˆ˜","ì—”í„°ë§¤ì¶œë¹„ìœ¨",
              "ì—”í„°ì „ì²´ë°©ë¬¸ììˆ˜","ì—”í„°ë°©ë¬¸ìë¹„ìœ¨","ì—”í„°í™œë™ë°€ë„","ì—”í„°ë§¤ì¶œë°€ë„"]
    X=(data[emo_vars].apply(pd.to_numeric,errors="coerce")
         .replace([np.inf,-np.inf],np.nan).dropna())
    if not X.empty:
        pc1=PCA(n_components=1).fit_transform(StandardScaler().fit_transform(X))
        data.loc[X.index,"FEEL_IDX"]=(pc1-pc1.min())/(pc1.max()-pc1.min()+1e-9)
    return data

data=preprocess()
st.title("ì„œìš¸ì‹œ ì¸ìŠ¤íƒ€ ê°ì„± ì§€ìˆ˜ ë¶„ì„")

if data.empty:
    st.error("ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    st.stop()

with st.sidebar:
    districts=st.multiselect("í–‰ì •ë™",sorted(data["DISTRICT_KOR_NAME"].dropna().unique()),[])
    age_groups=st.multiselect("ì—°ë ¹ëŒ€",sorted(data["AGE_GROUP"].unique()),[])
    gender=st.multiselect("ì„±ë³„",["M","F"],[])

mask=((data["DISTRICT_KOR_NAME"].isin(districts) if districts else True) &
      (data["AGE_GROUP"].isin(age_groups)        if age_groups else True) &
      (data["GENDER"].isin(gender)              if gender else True))
view=data.loc[mask]

c1,c2,c3=st.columns(3)
c1.metric("í‰ê·  FEEL_IDX",f"{view['FEEL_IDX'].mean():.2f}")
c2.metric("í‰ê·  ì†Œë¹„í™œë ¥ì§€ìˆ˜",f"{view['ì†Œë¹„í™œë ¥ì§€ìˆ˜'].mean():.2f}")
c3.metric("í‰ê·  ìœ ì…ì§€ìˆ˜",f"{view['ìœ ì…ì§€ìˆ˜'].mean():.2f}")

tab1,tab2,tab3=st.tabs(["ì§€ìˆ˜ ìƒìœ„ ì§€ì—­","ì„±ë³„Â·ì—°ë ¹ ë¶„ì„","ì‚°ì ë„"])

with tab1:
    top=view.groupby("DISTRICT_KOR_NAME")["ì†Œë¹„í™œë ¥ì§€ìˆ˜"].mean().nlargest(20)
    fig,ax=plt.subplots(figsize=(10,5))
    sns.barplot(x=top.index,y=top.values,palette="rocket",ax=ax)
    ax.set_xticklabels(ax.get_xticklabels(),rotation=45,ha="right")
    ax.set_xlabel("í–‰ì •ë™"); ax.set_ylabel("ì†Œë¹„í™œë ¥ì§€ìˆ˜"); st.pyplot(fig)

with tab2:
    agg=view.groupby(["AGE_GROUP","GENDER"])["TOTAL_SALES"].mean().reset_index()
    fig,ax=plt.subplots(figsize=(8,4))
    sns.barplot(data=agg,x="AGE_GROUP",y="TOTAL_SALES",hue="GENDER",
                palette={"M":"#3498db","F":"#e75480"},ax=ax); st.pyplot(fig)

with tab3:
    x=st.selectbox("Xì¶•",["ì—”í„°ì „ì²´ë§¤ì¶œ","ì†Œë¹„í™œë ¥ì§€ìˆ˜","ìœ ì…ì§€ìˆ˜","ì—”í„°ì „ì²´ë°©ë¬¸ììˆ˜"])
    y=st.selectbox("Yì¶•",["FEEL_IDX","ì—”í„°í™œë™ë°€ë„","ì—”í„°ë§¤ì¶œë¹„ìœ¨"])
    fig,ax=plt.subplots(figsize=(6,4))
    sns.scatterplot(data=view,x=x,y=y,hue="FEEL_IDX",palette="viridis",alpha=0.6,ax=ax)
    st.pyplot(fig)

st.divider(); st.caption("ë°ì´í„° ì¶œì²˜ Â· Snowflake Marketplace")
