import os
import streamlit as st
import snowflake.connector
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

# health-check íƒ€ì„ì•„ì›ƒ ë„ê¸°
os.environ["STREAMLIT_SERVER_HEALTH_CHECK_ENABLED"] = "false"

# ê¸°ë³¸ ì„¤ì •
st.set_page_config(page_title="ì„œìš¸ì‹œ ê°ì„± ì§€ìˆ˜ ëŒ€ì‹œë³´ë“œ", layout="wide")
sns.set_style("whitegrid")

# í•œê¸€ í°íŠ¸
import matplotlib
matplotlib.rcParams["font.family"] = "Malgun Gothic"
matplotlib.rcParams["axes.unicode_minus"] = False

st.markdown(
    '<style>*{font-family:"Malgun Gothic",sans-serif!important;}</style>',
    unsafe_allow_html=True,
)
st.write("ğŸš€ Streamlit ì•± ì‹œì‘!")

# â”€â”€ Snowflake ì—°ê²° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def get_conn():
    return snowflake.connector.connect(
        user=st.secrets["snowflake"]["user"],
        password=st.secrets["snowflake"]["password"],
        account=st.secrets["snowflake"]["account"],
        warehouse="COMPUTE_WH",
        ocsp_fail_open=True,
        insecure_mode=True,
    )

@st.cache_data(show_spinner=False)
def load_query(q: str) -> pd.DataFrame:
    conn = get_conn()
    cur  = conn.cursor()
    cur.execute(q)
    df = pd.DataFrame(cur.fetchall(), columns=[c[0] for c in cur.description])
    cur.close(); conn.close()
    return df

BASE = "SEOUL_DISTRICTLEVEL_DATA_FLOATING_POPULATION_CONSUMPTION_AND_ASSETS.GRANDATA"
Q_FP   = f"SELECT * FROM {BASE}.FLOATING_POPULATION_INFO LIMIT 8000"
Q_CARD = f"SELECT * FROM {BASE}.CARD_SALES_INFO           LIMIT 8000"
Q_ASSET= f"SELECT * FROM {BASE}.ASSET_INCOME_INFO         LIMIT 8000"
Q_SCCO = f"SELECT * FROM {BASE}.M_SCCO_MST"

@st.cache_data(show_spinner=True)
def load_all():
    return (
        load_query(Q_FP),
        load_query(Q_CARD),
        load_query(Q_ASSET),
        load_query(Q_SCCO),
    )

# â”€â”€ ì „ì²˜ë¦¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def preprocess() -> pd.DataFrame:
    fp, card, asset, scco = load_all()

    fp_card = pd.merge(
        fp, card,
        on=["STANDARD_YEAR_MONTH","DISTRICT_CODE","AGE_GROUP","GENDER",
            "TIME_SLOT","WEEKDAY_WEEKEND"],
        how="inner", validate="m:m"
    )
    fp_card_asset = pd.merge(
        fp_card, asset,
        on=["STANDARD_YEAR_MONTH","DISTRICT_CODE","AGE_GROUP","GENDER"],
        how="inner", validate="m:m"
    )
    dup_cols = [c for c in fp_card_asset.columns if c.endswith(("_x","_y"))]
    fp_card_asset = fp_card_asset.drop(columns=dup_cols)

    scco_map = (
        scco[["DISTRICT_CODE","DISTRICT_KOR_NAME"]]
        .drop_duplicates("DISTRICT_CODE")
        .set_index("DISTRICT_CODE")["DISTRICT_KOR_NAME"]
        .to_dict()
    )
    fp_card_asset["DISTRICT_KOR_NAME"] = fp_card_asset["DISTRICT_CODE"].map(scco_map)

    # â¬‡ í•„ìš”í•œ ë§Œí¼ë§Œ ìƒ˜í”Œë§ (êµì§‘í•© í›„)
    data = fp_card_asset.sample(frac=0.02, random_state=42)

    # íŒŒìƒ ë³€ìˆ˜
    data["ì „ì²´ì¸êµ¬"] = (
        data["RESIDENTIAL_POPULATION"] + data["WORKING_POPULATION"] + data["VISITING_POPULATION"]
    )
    data["ì—”í„°ì „ì²´ë§¤ì¶œ"] = (
        data["FOOD_SALES"] + data["COFFEE_SALES"] + data["BEAUTY_SALES"]
        + data["ENTERTAINMENT_SALES"] + data["SPORTS_CULTURE_LEISURE_SALES"]
        + data["TRAVEL_SALES"] + data["CLOTHING_ACCESSORIES_SALES"]
    )
    data["ì†Œë¹„í™œë ¥ì§€ìˆ˜"] = data["ì—”í„°ì „ì²´ë§¤ì¶œ"] / data["ì „ì²´ì¸êµ¬"].replace(0,np.nan)
    data["ìœ ì…ì§€ìˆ˜"] = data["VISITING_POPULATION"] / (
        data["RESIDENTIAL_POPULATION"] + data["WORKING_POPULATION"]
    ).replace(0,np.nan)
    data["ì—”í„°ë§¤ì¶œë¹„ìœ¨"] = data["ì—”í„°ì „ì²´ë§¤ì¶œ"] / data["TOTAL_SALES"].replace(0,np.nan)

    cnt_cols = [
        "FOOD_COUNT","COFFEE_COUNT","BEAUTY_COUNT","ENTERTAINMENT_COUNT",
        "SPORTS_CULTURE_LEISURE_COUNT","TRAVEL_COUNT","CLOTHING_ACCESSORIES_COUNT"
    ]
    data["ì—”í„°ì „ì²´ë°©ë¬¸ììˆ˜"] = data[cnt_cols].sum(axis=1)
    data["ì—”í„°ë°©ë¬¸ìë¹„ìœ¨"] = data["ì—”í„°ì „ì²´ë°©ë¬¸ììˆ˜"] / data["TOTAL_COUNT"].replace(0,np.nan)
    data["ì—”í„°í™œë™ë°€ë„"] = data["ì—”í„°ì „ì²´ë§¤ì¶œ"] / data["ì „ì²´ì¸êµ¬"].replace(0,np.nan)
    data["ì—”í„°ë§¤ì¶œë°€ë„"] = data["ì—”í„°ì „ì²´ë§¤ì¶œ"] / data["ì—”í„°ì „ì²´ë°©ë¬¸ììˆ˜"].replace(0,np.nan)

    emo_vars = [
        "ì—”í„°ì „ì²´ë§¤ì¶œ","ì†Œë¹„í™œë ¥ì§€ìˆ˜","ìœ ì…ì§€ìˆ˜","ì—”í„°ë§¤ì¶œë¹„ìœ¨",
        "ì—”í„°ì „ì²´ë°©ë¬¸ììˆ˜","ì—”í„°ë°©ë¬¸ìë¹„ìœ¨","ì—”í„°í™œë™ë°€ë„","ì—”í„°ë§¤ì¶œë°€ë„"
    ]
    X = (
        data[emo_vars]
        .apply(pd.to_numeric, errors="coerce")
        .replace([np.inf,-np.inf], np.nan)
        .dropna()
    )
    if X.empty:
        data["FEEL_IDX"] = np.nan
        return data

    scaler = StandardScaler().fit(X)
    pc1 = PCA(n_components=1).fit_transform(scaler.transform(X))
    pc1_n = (pc1 - pc1.min()) / (pc1.max()-pc1.min()+1e-9)
    data.loc[X.index,"FEEL_IDX"] = pc1_n
    return data

@st.cache_data(show_spinner=True)
def get_data():
    return preprocess()

# â”€â”€ UI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.title("ì„œìš¸ì‹œ ì¸ìŠ¤íƒ€ ê°ì„± ì§€ìˆ˜ ë¶„ì„")
data = get_data()

if data.empty:
    st.warning("ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ìƒ˜í”Œë§ ë¹„ìœ¨ì„ ë†’ì—¬ ë³´ì„¸ìš”.")
    st.stop()

with st.sidebar:
    districts = st.multiselect("í–‰ì •ë™", sorted(data["DISTRICT_KOR_NAME"].dropna().unique()), [])
    age_groups = st.multiselect("ì—°ë ¹ëŒ€", sorted(data["AGE_GROUP"].unique()), [])
    gender = st.multiselect("ì„±ë³„", ["M","F"], [])

mask = (
    (data["DISTRICT_KOR_NAME"].isin(districts) if districts else True) &
    (data["AGE_GROUP"].isin(age_groups)       if age_groups else True) &
    (data["GENDER"].isin(gender)             if gender else True)
)
view = data.loc[mask]

st.subheader("ìš”ì•½ ì§€í‘œ")
c1,c2,c3 = st.columns(3)
c1.metric("í‰ê·  FEEL_IDX", f"{view['FEEL_IDX'].mean():.2f}")
c2.metric("í‰ê·  ì†Œë¹„í™œë ¥ì§€ìˆ˜", f"{view['ì†Œë¹„í™œë ¥ì§€ìˆ˜'].mean():.2f}")
c3.metric("í‰ê·  ìœ ì…ì§€ìˆ˜", f"{view['ìœ ì…ì§€ìˆ˜'].mean():.2f}")

tab1,tab2,tab3 = st.tabs(["ì§€ìˆ˜ ìƒìœ„ ì§€ì—­","ì„±ë³„Â·ì—°ë ¹ ë¶„ì„","ì‚°ì ë„"])

with tab1:
    top = (
        view.groupby("DISTRICT_KOR_NAME")["ì†Œë¹„í™œë ¥ì§€ìˆ˜"]
        .mean()
        .sort_values(ascending=False)
        .head(20)
    )
    fig,ax = plt.subplots(figsize=(10,5))
    sns.barplot(x=top.index, y=top.values, palette="rocket", ax=ax)
    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha="right")
    ax.set_xlabel("í–‰ì •ë™"); ax.set_ylabel("ì†Œë¹„í™œë ¥ì§€ìˆ˜")
    st.pyplot(fig)

with tab2:
    agg = (
        view.groupby(["AGE_GROUP","GENDER"])["TOTAL_SALES"]
        .mean().reset_index()
    )
    fig,ax = plt.subplots(figsize=(8,4))
    sns.barplot(data=agg, x="AGE_GROUP", y="TOTAL_SALES", hue="GENDER",
                palette={"M":"#3498db","F":"#e75480"}, ax=ax)
    st.pyplot(fig)

with tab3:
    x_axis = st.selectbox("Xì¶• ë³€ìˆ˜", ["ì—”í„°ì „ì²´ë§¤ì¶œ","ì†Œë¹„í™œë ¥ì§€ìˆ˜","ìœ ì…ì§€ìˆ˜","ì—”í„°ì „ì²´ë°©ë¬¸ììˆ˜"])
    y_axis = st.selectbox("Yì¶• ë³€ìˆ˜", ["FEEL_IDX","ì—”í„°í™œë™ë°€ë„","ì—”í„°ë§¤ì¶œë¹„ìœ¨"])
    fig,ax = plt.subplots(figsize=(6,4))
    sns.scatterplot(data=view, x=x_axis, y=y_axis,
                    hue="FEEL_IDX", palette="viridis", alpha=0.6, ax=ax)
    st.pyplot(fig)

st.divider()
st.caption("ë°ì´í„° ì¶œì²˜ Â· Snowflake Marketplace")
